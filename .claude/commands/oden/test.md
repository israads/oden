---
allowed-tools: Bash, Read, Write, Task, Glob, Grep
description: Sistema de testing completo con generaciÃ³n automÃ¡tica y auto-fix - testing robusto basado en lÃ³gica de negocio
---

# Test - Sistema de Testing Completo con Auto-Fix

Sistema completo de testing que genera tests automÃ¡ticamente basados en specs y lÃ³gica de negocio, ejecuta tests y auto-corrige fallos.

## Usage

```bash
# Generar tests de mÃ³dulo especÃ­fico
/oden:test <module-name>

# Generar tests de todo el sistema (con confirmaciÃ³n)
/oden:test

# Ejecutar tests con auto-fix si fallan
/oden:test run

# Generar estrategia de testing
/oden:test strategy

# Analizar cobertura de tests
/oden:test coverage

# Generar tests desde specs
/oden:test generate <module-name>
```

## ğŸ¯ PropÃ³sito: Testing Enterprise-Grade

Sistema que combina:
- âœ… **Test Generation**: AutomÃ¡tica desde specs y lÃ³gica de negocio
- âœ… **Smart Framework Selection**: Vitest, Jest, Cypress segÃºn el stack
- âœ… **Auto-Fix**: CorrecciÃ³n automÃ¡tica de tests fallidos
- âœ… **Business Logic Aware**: Tests que entienden la lÃ³gica de negocio
- âœ… **Spec-Driven**: Tests generados desde especificaciones tÃ©cnicas

## ğŸ“Š Comando: /oden:test (Generate All Tests)

### Preflight Validation

1. **Check for existing tests**: Detectar estructura de tests existente
2. **Identify framework**: Auto-detectar o sugerir framework de testing
3. **Read project specs**: Leer specs disponibles para contexto
4. **Get datetime**: `date -u +"%Y-%m-%dT%H:%M:%SZ"` for test generation timestamp

### User Confirmation for Full System

```bash
echo "ğŸ§ª ODEN FORGE - SISTEMA DE TESTING COMPLETO"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "ğŸ“Š AnÃ¡lisis del proyecto:"
echo "  - MÃ³dulos detectados: [count] mÃ³dulos"
echo "  - Specs disponibles: [count] especificaciones"
echo "  - Framework sugerido: [Vitest|Jest|Cypress] (auto-detectado)"
echo "  - Cobertura objetivo: 85%+"
echo ""
echo "ğŸš¨ ADVERTENCIA: Esto generarÃ¡ tests para TODO el sistema"
echo "   Esto puede tomar 15-30 minutos y crear muchos archivos"
echo ""
echo "Â¿Proceder con generaciÃ³n completa de tests? [y/N]: "
```

### Phase 1: Test Strategy & Framework Selection

Launch specialized subagent for test strategy:

```markdown
Launch subagent: test-engineer

Task: Analyze project and create comprehensive test strategy

Requirements:
- Read all specs in docs/reference/modules/
- Read technical-decisions.md for stack information
- Analyze existing codebase structure (if accessible)
- Identify business logic patterns
- Recommend optimal test framework:
  * Vitest for Vite/Vue projects
  * Jest for React/Node projects
  * Cypress for E2E testing
  * Playwright for modern E2E
  * Framework-specific options for other stacks

- Create test architecture plan:
  * Unit tests for business logic
  * Integration tests for API endpoints
  * E2E tests for user workflows
  * Performance tests for critical paths

- Output comprehensive test strategy with framework rationale

Context: Generate enterprise-grade testing strategy based on actual project needs
```

### Phase 2: Module-by-Module Test Generation

For each module, launch specialized test generation:

```markdown
Launch subagent: test-generator

Task: Generate comprehensive tests for module: $MODULE_NAME

Requirements:
- Read module spec: docs/reference/modules/$MODULE_NAME-spec.md
- Read related PRDs and epics for business context
- Understand business logic and edge cases
- Generate test files following framework best practices:
  * Unit tests for all functions/methods
  * Integration tests for module interactions
  * Mock external dependencies appropriately
  * Test error handling and edge cases
  * Include performance tests where relevant

- Follow naming conventions:
  * $MODULE_NAME.test.js/ts for unit tests
  * $MODULE_NAME.integration.test.js for integration
  * $MODULE_NAME.e2e.test.js for end-to-end

- Include test data factories and fixtures
- Generate comprehensive test coverage (target 85%+)
- Output working test files ready for execution

Context: Tests must validate business logic, not just code coverage
```

## ğŸ”§ Comando: /oden:test <module-name> (Generate Module Tests)

### Module-Specific Test Generation

```bash
# Validate module exists
if [ ! -f "docs/reference/modules/$ARGUMENTS-spec.md" ]; then
  echo "âŒ Module spec not found: docs/reference/modules/$ARGUMENTS-spec.md"
  echo "Available modules:"
  ls docs/reference/modules/ | grep -E ".*-spec\.md$" | sed 's/-spec\.md$//' | sed 's/^/  - /'
  exit 1
fi
```

### Single Module Test Strategy

```markdown
Launch subagent: test-generator

Task: Generate comprehensive tests for module: $ARGUMENTS

Requirements:
- Read module spec: docs/reference/modules/$ARGUMENTS-spec.md
- Read technical-decisions.md for tech stack context
- Identify business logic patterns in the spec
- Generate appropriate test categories:
  * Unit tests for core business logic
  * Integration tests for external interactions
  * Mock tests for dependencies
  * Edge case tests for error conditions

- Select optimal framework based on stack:
  * Auto-detect package.json dependencies
  * Suggest Vitest for modern JavaScript/TypeScript
  * Include proper setup and teardown
  * Generate test utilities and helpers

- Create test files with proper structure:
  * Clear test descriptions
  * Arrange-Act-Assert pattern
  * Comprehensive test data
  * Business scenario coverage

- Output summary of test coverage and rationale

Context: Focus on testing business logic, not just implementation details
```

### Expected Output Structure

Create test files in appropriate directory structure:
```
tests/
â”œâ”€â”€ unit/
â”‚   â””â”€â”€ $MODULE_NAME.test.js
â”œâ”€â”€ integration/
â”‚   â””â”€â”€ $MODULE_NAME.integration.test.js
â”œâ”€â”€ fixtures/
â”‚   â””â”€â”€ $MODULE_NAME-data.js
â””â”€â”€ helpers/
    â””â”€â”€ $MODULE_NAME-helpers.js
```

## ğŸš€ Comando: /oden:test run (Execute Tests with Auto-Fix)

### Test Execution Pipeline

#### Phase 1: Framework Detection & Execution

```bash
echo "ğŸ§ª Executing tests with auto-fix capability..."
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# Auto-detect testing framework and execute
test_framework=""
test_command=""

# Detect framework
if [ -f "package.json" ]; then
  if grep -q "vitest" package.json; then
    test_framework="vitest"
    test_command="npm run test"
  elif grep -q "jest" package.json; then
    test_framework="jest"
    test_command="npm test"
  elif grep -q "cypress" package.json; then
    test_framework="cypress"
    test_command="npm run test:e2e"
  fi
elif [ -f "go.mod" ]; then
  test_framework="go-test"
  test_command="go test ./..."
elif [ -f "Cargo.toml" ]; then
  test_framework="cargo"
  test_command="cargo test"
elif [ -f "pubspec.yaml" ]; then
  test_framework="flutter"
  test_command="flutter test"
elif [ -f "Gemfile" ]; then
  test_framework="rspec"
  test_command="bundle exec rspec"
fi

echo "ğŸ“Š Testing Framework: $test_framework"
echo "ğŸ”§ Command: $test_command"
echo ""

# Execute tests
echo "â–¶ï¸ Running tests..."
$test_command > /tmp/test-output.log 2>&1
test_exit_code=$?
```

#### Phase 2: Auto-Fix Failed Tests

If tests fail (exit code != 0):

```bash
if [ $test_exit_code -ne 0 ]; then
  echo "âŒ Tests failed (exit code: $test_exit_code)"
  echo ""
  echo "ğŸ” Analyzing failures for auto-fix..."

  # Launch auto-fix subagent
fi
```

```markdown
Launch subagent: test-fixer

Task: Analyze test failures and automatically fix them

Requirements:
- Read full test output from /tmp/test-output.log
- Read all relevant specs and business logic documentation
- Read technical-decisions.md for context
- Analyze failure patterns:
  * Import/export errors â†’ Fix path issues
  * Missing dependencies â†’ Add proper mocks
  * Business logic mismatches â†’ Align with specs
  * Type errors â†’ Fix TypeScript issues
  * Async/timing issues â†’ Add proper awaits

- Apply fixes based on failure analysis:
  * Update import paths
  * Fix mock configurations
  * Correct business logic in tests
  * Add missing test setup
  * Fix async test patterns

- Re-run tests after each fix to validate
- Continue until all tests pass or manual intervention needed
- Document what was fixed and why

Context: Fix tests while maintaining business logic correctness and spec compliance
```

#### Phase 3: Success Reporting

```bash
if [ $final_exit_code -eq 0 ]; then
  echo "âœ… All tests passing after auto-fix!"
else
  echo "âš ï¸ Some tests still failing - manual intervention needed"
fi
```

## ğŸ“ˆ Comando: /oden:test coverage (Coverage Analysis)

### Coverage Analysis with Business Logic Insights

```markdown
Launch subagent: test-analyzer

Task: Analyze test coverage and business logic coverage

Requirements:
- Execute coverage tools (nyc, jest --coverage, etc.)
- Read all module specs to understand business requirements
- Analyze coverage report against business logic:
  * Which business rules are tested
  * Which edge cases are missing
  * What error conditions are uncovered
  * Which integration points need testing

- Generate business-focused coverage report:
  * Overall coverage percentage
  * Business logic coverage (separate metric)
  * Critical path coverage
  * Error handling coverage
  * Integration coverage

- Suggest specific tests to improve meaningful coverage
- Prioritize recommendations by business impact

Context: Focus on business value, not just code coverage percentage
```

## ğŸ¯ Comando: /oden:test strategy (Generate Test Strategy)

### Comprehensive Test Strategy Creation

```markdown
Launch subagent: test-architect

Task: Create comprehensive testing strategy for the project

Requirements:
- Read all project documentation (specs, technical decisions, PRDs)
- Analyze codebase architecture and patterns
- Understand business domain and critical workflows
- Create multi-layered test strategy:
  * Unit Testing Strategy
  * Integration Testing Strategy
  * E2E Testing Strategy
  * Performance Testing Strategy
  * Security Testing Strategy

- Define testing standards:
  * Coverage targets by layer
  * Testing naming conventions
  * Mock strategy and guidelines
  * CI/CD integration approach
  * Test data management

- Create testing roadmap:
  * Phase 1: Critical path tests
  * Phase 2: Comprehensive coverage
  * Phase 3: Advanced scenarios
  * Maintenance and evolution plan

Output: Comprehensive test strategy document

Context: Create professional testing strategy aligned with business needs
```

## ğŸ“‹ Integration with Development Pipeline

### From /oden:work (Enhanced)

Add automatic testing integration to work orchestrator:

```bash
# After all development streams complete
echo ""
echo "ğŸ§ª Running automatic test verification..."
/oden:test run

# If tests fail, auto-fix attempt
if [ $? -ne 0 ]; then
  echo "âš ï¸ Tests failed - attempting auto-fix..."
  /oden:test run --auto-fix
fi

# Only proceed to PR creation if tests pass
if [ $? -eq 0 ]; then
  echo "âœ… All tests passing - ready for PR creation"
else
  echo "âŒ Tests still failing - manual intervention required"
  exit 1
fi
```

### From /oden:epic (Test Planning)

Add test planning to epic creation:

```bash
# After epic validation passes
echo ""
echo "ğŸ§ª Generating test strategy for epic..."

# Launch test planning subagent
```

```markdown
Launch subagent: test-planner

Task: Create testing plan for epic: $ARGUMENTS

Requirements:
- Read epic document: .claude/epics/$ARGUMENTS/epic.md
- Understand work streams and technical implementation
- Plan testing approach for each work stream:
  * Database changes â†’ migration and schema tests
  * API development â†’ endpoint and integration tests
  * Frontend work â†’ component and E2E tests
  * Infrastructure â†’ deployment and performance tests

- Estimate testing effort per work stream
- Identify test dependencies and sequencing
- Plan test data requirements
- Output testing tasks to be added to work streams

Context: Integrate testing into development workflow from planning stage
```

## ğŸ”§ Framework-Specific Implementations

### Vitest Configuration

If Vitest is detected/selected:

```javascript
// vitest.config.js
import { defineConfig } from 'vitest/config'

export default defineConfig({
  test: {
    environment: 'jsdom', // or 'node' based on needs
    coverage: {
      provider: 'v8',
      reporter: ['text', 'html', 'lcov'],
      thresholds: {
        global: {
          branches: 85,
          functions: 85,
          lines: 85,
          statements: 85
        }
      }
    },
    setupFiles: ['./test/setup.js'],
    globals: true
  }
})
```

### Jest Configuration

If Jest is detected/selected:

```javascript
// jest.config.js
module.exports = {
  testEnvironment: 'jsdom',
  coverageThreshold: {
    global: {
      branches: 85,
      functions: 85,
      lines: 85,
      statements: 85
    }
  },
  setupFilesAfterEnv: ['<rootDir>/test/setup.js'],
  testMatch: ['**/__tests__/**/*.(js|ts)', '**/*.(test|spec).(js|ts)']
}
```

## ğŸ“Š Success Output Examples

### Module Test Generation
```
ğŸ§ª Module Tests Generated: $MODULE_NAME

ğŸ“‹ Test Files Created:
  - tests/unit/$MODULE_NAME.test.js (15 tests)
  - tests/integration/$MODULE_NAME.integration.test.js (8 tests)
  - tests/fixtures/$MODULE_NAME-data.js (test data)

ğŸ“Š Coverage Focus:
  - Business logic: 12 core functions tested
  - Edge cases: 8 error conditions covered
  - Integration points: 5 external dependencies mocked
  - Performance: 3 critical path benchmarks

ğŸ¯ Business Scenarios Covered:
  - User authentication flow (5 tests)
  - Payment processing (7 tests)
  - Error handling (6 tests)

â–¶ï¸ Run tests: /oden:test run
ğŸ“ˆ Check coverage: /oden:test coverage
```

### Test Run with Auto-Fix
```
ğŸ§ª Test Execution Complete with Auto-Fix

ğŸ“Š Results:
  - Total tests: 127
  - Passed: 127 âœ…
  - Failed: 0
  - Auto-fixed: 3 issues

ğŸ”§ Auto-Fixes Applied:
  - Import path corrections (2 files)
  - Mock configuration update (1 file)
  - Async test timing fix (1 file)

ğŸ“ˆ Coverage: 87% (exceeds 85% target)
  - Business logic: 92%
  - Integration: 84%
  - E2E workflows: 89%

âœ… All tests passing - ready for development/deployment
```

### Coverage Analysis
```
ğŸ“Š Test Coverage Analysis

Overall Coverage: 87%
â”œâ”€ Business Logic: 92% âœ…
â”œâ”€ Integration: 84% âš ï¸
â”œâ”€ E2E Workflows: 89% âœ…
â””â”€ Error Handling: 78% âš ï¸

ğŸ¯ Business Priority Coverage:
  - Authentication: 95% âœ…
  - Payment Flow: 91% âœ…
  - User Management: 89% âœ…
  - Data Processing: 82% âš ï¸

ğŸ“‹ Recommendations (High Impact):
  1. Add error handling tests for payment failures
  2. Test integration with external API timeouts
  3. Add E2E tests for admin workflows

âš¡ Quick wins available:
  - 3 tests could boost coverage to 90%
  - Est. time: 2 hours
```

## âš ï¸ Error Handling & Recovery

### Common Issues & Auto-Fixes

#### Framework Detection Failure
```bash
if [ -z "$test_framework" ]; then
  echo "âŒ Could not detect testing framework"
  echo ""
  echo "ğŸ”§ Suggested frameworks based on your stack:"
  # Auto-suggest based on package.json or file structure
  echo "   - Vitest (recommended for modern projects)"
  echo "   - Jest (standard for React/Node)"
  echo ""
  echo "Install testing framework? [y/N]: "
fi
```

#### Test Generation Failure
```bash
if [ $generation_failed -eq 1 ]; then
  echo "âŒ Test generation failed for $MODULE_NAME"
  echo ""
  echo "ğŸ” Possible causes:"
  echo "   - Module spec missing or incomplete"
  echo "   - Complex business logic needs manual intervention"
  echo "   - Framework setup incomplete"
  echo ""
  echo "ğŸ“‹ Next steps:"
  echo "   1. Check spec: docs/reference/modules/$MODULE_NAME-spec.md"
  echo "   2. Verify framework setup"
  echo "   3. Generate basic test template manually"
fi
```

#### Auto-Fix Limitations
```bash
if [ $auto_fix_failed -eq 1 ]; then
  echo "âš ï¸ Auto-fix could not resolve all test failures"
  echo ""
  echo "ğŸ” Manual intervention needed for:"
  # List specific failures that couldn't be auto-fixed
  echo "   - Complex business logic mismatch (requires spec review)"
  echo "   - External service integration (needs environment setup)"
  echo ""
  echo "ğŸ“‹ Guidance:"
  echo "   1. Review failing tests in context of business specs"
  echo "   2. Check external dependencies and mocks"
  echo "   3. Verify test environment configuration"
fi
```

---

**ğŸ§ª El sistema de testing de Oden Forge garantiza cobertura enterprise-grade con generaciÃ³n automÃ¡tica inteligente, auto-fix capabilities, y enfoque en lÃ³gica de negocio real, no solo coverage percentages.**